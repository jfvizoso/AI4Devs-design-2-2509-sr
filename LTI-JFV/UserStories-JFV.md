# User Stories & Backlog - LTI ATS

## 1. User Stories: AI-Driven Sourcing & Screening

**Story 1: Automated Candidate Sourcing**
- **As a:** Recruiter
- **I want to:** Have the system automatically scan external platforms (LinkedIn, GitHub) for candidates matching a job description
- **So that:** I can identify passive talent without spending hours on manual searching.
- **Acceptance Criteria:**
  - [ ] System accepts a Job Description as input.
  - [ ] System scans at least 2 external sources (LinkedIn, GitHub).
  - [ ] System returns a list of at least 50 potential candidates.
  - [ ] System respects `robots.txt` and rate limits of external sites.

**Story 2: Semantic Fit Score Ranking**
- **As a:** Recruiter
- **I want to:** See candidates ranked by a "Fit Score" based on semantic analysis of their skills and experience
- **So that:** I can prioritize reviewing the most relevant profiles first and reduce time-to-hire.
- **Acceptance Criteria:**
  - [ ] System parses candidate resumes and job descriptions into vector embeddings.
  - [ ] System calculates a cosine similarity score (0-100%) for each candidate.
  - [ ] Candidates are displayed in descending order of Fit Score.
  - [ ] UI highlights key matching skills and "missing" skills.

**Story 3: Smart Chatbot Screening**
- **As a:** Candidate
- **I want to:** Complete an initial screening interview via a WhatsApp/SMS chatbot immediately after applying
- **So that:** I can progress in the application process 24/7 without waiting for a recruiter to schedule a call.
- **Acceptance Criteria:**
  - [ ] Chatbot initiates contact within 5 minutes of application (or sourcing).
  - [ ] Chatbot asks 3 configurable "knockout" questions (e.g., Visa status, Salary expectations).
  - [ ] Chatbot parses natural language responses to determine pass/fail.
  - [ ] If passed, Chatbot offers interview slots from the recruiter's calendar.

---

## 2. Product Backlog Generation Experiments

### Experiment 1: Standard Prioritization (Business Value)

| ID | Title | Priority | Rationale |
| :--- | :--- | :--- | :--- |
| US-1 | Automated Candidate Sourcing | **High** | Core "Unfair Advantage" of LTI. Directly addresses the "Inefficient Processes" problem. |
| US-2 | Semantic Fit Score Ranking | **High** | Essential for managing the volume of candidates generated by US-1. |
| US-3 | Smart Chatbot Screening | **Medium** | High impact on candidate experience, but US-1 & US-2 are prerequisites for generating the pipeline. |

### Experiment 2: MoSCoW Method

*   **Must Have (Critical for MVP):**
    *   **Automated Candidate Sourcing:** Without this, LTI is just a standard database. This is the "Talent Engine."
    *   **Semantic Fit Score Ranking:** Necessary to make the sourced data usable.
*   **Should Have (Important but not vital for launch):**
    *   **Smart Chatbot Screening:** Great differentiator, but manual screening could suffice for V1 alpha.
*   **Could Have (Desirable):**
    *   *Async Video Debriefs (from PRD):* Nice feature, but adds complexity to the MVP.
*   **Won't Have (For now):**
    *   *Predictive Attrition Analysis:* Requires large historical datasets that won't exist at launch.

### Experiment 3: Value vs. Effort Matrix

| User Story | Business Value | Estimated Effort | Classification |
| :--- | :--- | :--- | :--- |
| **Semantic Fit Score Ranking** | High | Medium | **Quick Win** (Leverage existing Vector DBs) |
| **Automated Candidate Sourcing** | High | High | **Big Bet** (Complex scraping & compliance) |
| **Smart Chatbot Screening** | Medium | Low | **Quick Win** (LLM APIs make this easy) |

### Conclusion: Winner & Rationale
**Winner:** **Experiment 3 (Value vs. Effort)**.
**Rationale:** While MoSCoW is good for scope, the Value/Effort matrix is more effective for an AI project where "Effort" can vary wildly (e.g., scraping is hard, LLM chat is easy). It highlighted that the **Smart Chatbot** is actually a "Quick Win" (Low Effort, Medium Value) and might be worth prioritizing earlier than the MoSCoW method suggested, simply because it's easy to build with modern LLMs and delivers immediate "Wow" factor.

---

## 3. Work Tickets (Selected Story: Automated Candidate Sourcing)

**User Story:** As a Recruiter, I want the system to automatically scan external platforms...

| Ticket ID | Title | Type | Description | Acceptance Criteria |
| :--- | :--- | :--- | :--- | :--- |
| **LTI-101** | **Implement LinkedIn/GitHub Scraper Agent** | Backend | Create a Python service using `BeautifulSoup` or `Selenium` (or official APIs if available) to fetch public profile data based on search criteria. **Tech:** Python, Selenium/Playwright. | 1. Service accepts search parameters (Role, Location).<br>2. Service returns JSON list of profiles.<br>3. Implements exponential backoff to avoid IP bans. |
| **LTI-102** | **Develop Requirement Analyzer (JD to Vector)** | AI/ML | Create a pipeline to parse Job Descriptions, extract key skills using OpenAI, and generate a search query vector. **Tech:** Python, OpenAI API, LangChain. | 1. Input: Raw text JD.<br>2. Output: Structured JSON (Skills, YOE) + Vector Embedding.<br>3. Unit tests with sample JDs. |
| **LTI-103** | **Create Candidate Ingestion Pipeline** | Backend | Build a Node.js consumer that takes raw profiles from the Scraper, normalizes the data, and saves it to PostgreSQL and Pinecone. **Tech:** Node.js, PostgreSQL, Pinecone. | 1. Idempotent storage (no duplicates).<br>2. Data validation (email/phone format).<br>3. Async processing via RabbitMQ/Kafka. |
| **LTI-104** | **Design Candidate Vector Schema** | Database | Define the Pinecone index schema and PostgreSQL schema for storing candidate metadata and embeddings. **Tech:** PostgreSQL, Pinecone. | 1. SQL Migration script created.<br>2. Pinecone index configured (Dimension: 1536 for Ada-002). |

---

## 4. Effort Estimation

**Methodology:** Planning Poker (Fibonacci)

| Ticket ID | Title | Estimate (Points) | Reasoning |
| :--- | :--- | :--- | :--- |
| **LTI-101** | Scraper Agent | **13** | **High Uncertainty.** External sites change DOM frequently; anti-bot detection is a major risk. Requires robust error handling and proxy management. |
| **LTI-102** | Requirement Analyzer | **5** | **Straightforward.** OpenAI API handles the heavy lifting of extraction. Main work is prompt engineering. |
| **LTI-103** | Ingestion Pipeline | **8** | **Moderate Complexity.** Handling data normalization from unstructured sources is messy. ensuring idempotency across distributed events adds effort. |
| **LTI-104** | Vector Schema | **3** | **Low Complexity.** Standard schema definition. Pinecone setup is trivial. |

**Total Effort for Story:** 29 Points (A significant epic, likely needs breaking down further).
